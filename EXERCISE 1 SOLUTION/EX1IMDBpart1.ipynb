{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6e34623-0c5e-4c0f-bb6e-3f3465d98405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      nconst      primaryName birthYear deathYear  \\\n",
      "0  nm0000001     Fred Astaire      1899      1987   \n",
      "1  nm0000002    Lauren Bacall      1924      2014   \n",
      "2  nm0000003  Brigitte Bardot      1934        \\N   \n",
      "3  nm0000004     John Belushi      1949      1982   \n",
      "4  nm0000005   Ingmar Bergman      1918      2007   \n",
      "\n",
      "                    primaryProfession                           knownForTitles  \n",
      "0        actor,miscellaneous,producer  tt0072308,tt0050419,tt0053137,tt0027125  \n",
      "1  actress,soundtrack,archive_footage  tt0037382,tt0075213,tt0117057,tt0038355  \n",
      "2   actress,music_department,producer  tt0057345,tt0049189,tt0056404,tt0054452  \n",
      "3       actor,writer,music_department  tt0072562,tt0077975,tt0080455,tt0078723  \n",
      "4               writer,director,actor  tt0050986,tt0083922,tt0050976,tt0069467  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# We will first load the data into the Bronze layer \n",
    "import pandas as pd\n",
    "\n",
    "# Load the name.basics file\n",
    "file_path = r'C:\\Users\\Dell E7440\\Desktop\\BRONZEIMDB\\name.basics.tsv'  \n",
    "name_basics_df = pd.read_csv(file_path, sep='\\t') \n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(name_basics_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cf0594f-08b6-4a11-9c83-3a6c26e63859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nconst                      0\n",
      "primaryName                58\n",
      "birthYear            13233613\n",
      "deathYear            13626356\n",
      "primaryProfession     2683536\n",
      "knownForTitles        1558132\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#As in the kaggle website is said that A ‘\\N’ is used to denote that a particular field is missing or null for that title/name:\n",
    "import pandas as pd\n",
    "\n",
    "# Load the name.basics file\n",
    "file_path = r'C:\\Users\\Dell E7440\\Desktop\\BRONZEIMDB\\name.basics.tsv'\n",
    "name_basics_df = pd.read_csv(file_path, sep='\\t', na_values='\\\\N', encoding='utf-8')\n",
    "\n",
    "# Count the null values in the DataFrame\n",
    "null_counts = name_basics_df.isnull().sum()\n",
    "\n",
    "# Display the null counts for each column\n",
    "print(null_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29781574-2fc7-4c7e-a9d7-57334a5ac3a3",
   "metadata": {},
   "source": [
    "SILVER LAYER TRANSFORMATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf621202-f72e-4566-b30d-4d61d23dd02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after dropping nulls: 13861238\n",
      "Cleaned data saved to: C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_cleaned.tsv\n"
     ]
    }
   ],
   "source": [
    "#Lets handle the null values in the primaryName column, we will drop them as thay are quite afew.\n",
    "import pandas as pd\n",
    "\n",
    "# Load the name.basics file\n",
    "file_path = r'C:\\Users\\Dell E7440\\Desktop\\BRONZEIMDB\\name.basics.tsv'\n",
    "name_basics_df = pd.read_csv(file_path, sep='\\t', na_values='\\\\N', encoding='utf-8')\n",
    "\n",
    "# Drop rows where primaryName is null\n",
    "name_basics_cleaned_df = name_basics_df.dropna(subset=['primaryName'])\n",
    "\n",
    "# Display the number of rows after dropping nulls\n",
    "print(f'Number of rows after dropping nulls: {name_basics_cleaned_df.shape[0]}')\n",
    "\n",
    "# Save the cleaned DataFrame to the Silver layer\n",
    "cleaned_file_path = r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_cleaned.tsv'\n",
    "name_basics_cleaned_df.to_csv(cleaned_file_path, sep='\\t', index=False)\n",
    "\n",
    "print(f'Cleaned data saved to: {cleaned_file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e980719b-548e-4ddd-8f4e-5bee2cb949f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null counts after cleaning:\n",
      "nconst                      0\n",
      "primaryName                 0\n",
      "birthYear            13233555\n",
      "deathYear            13626298\n",
      "primaryProfession     2683485\n",
      "knownForTitles        1558080\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned name.basics file\n",
    "cleaned_file_path = r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_cleaned.tsv'\n",
    "name_basics_cleaned_df = pd.read_csv(cleaned_file_path, sep='\\t', encoding='utf-8')\n",
    "\n",
    "# Check for null values in the cleaned DataFrame\n",
    "null_counts_after_cleaning = name_basics_cleaned_df.isnull().sum()\n",
    "\n",
    "# Display the null counts for each column\n",
    "print('Null counts after cleaning:')\n",
    "print(null_counts_after_cleaning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b0d9072-9bfc-4c47-a995-ed2f33b80653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types of the columns:\n",
      "nconst                object\n",
      "primaryName           object\n",
      "birthYear            float64\n",
      "deathYear            float64\n",
      "primaryProfession     object\n",
      "knownForTitles        object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Check for the type of the columns and turn birth and death year to an integer\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned name.basics file\n",
    "cleaned_file_path = r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_cleaned.tsv'\n",
    "name_basics_cleaned_df = pd.read_csv(cleaned_file_path, sep='\\t', encoding='utf-8', na_values='\\\\N')\n",
    "\n",
    "# Check data types of the columns\n",
    "print('Data types of the columns:')\n",
    "print(name_basics_cleaned_df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c55f9020-6967-4c41-a8d1-464875cb57a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types after conversion:\n",
      "nconst               object\n",
      "primaryName          object\n",
      "birthYear             Int64\n",
      "deathYear             Int64\n",
      "primaryProfession    object\n",
      "knownForTitles       object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert birthYear and deathYear to integers\n",
    "name_basics_cleaned_df['birthYear'] = name_basics_cleaned_df['birthYear'].astype('Int64')  # Using 'Int64' to handle NA\n",
    "name_basics_cleaned_df['deathYear'] = name_basics_cleaned_df['deathYear'].astype('Int64')  # Using 'Int64' to handle NA\n",
    "\n",
    "# Check data types again to confirm changes\n",
    "print('Data types after conversion:')\n",
    "print(name_basics_cleaned_df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf44f160-459b-465f-a4bc-8c652f6d0040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploded DataFrame shape: (18157649, 6)\n",
      "      nconst    primaryName  birthYear  deathYear primaryProfession  \\\n",
      "0  nm0000001   Fred Astaire     1899.0     1987.0             actor   \n",
      "1  nm0000001   Fred Astaire     1899.0     1987.0     miscellaneous   \n",
      "2  nm0000001   Fred Astaire     1899.0     1987.0          producer   \n",
      "3  nm0000002  Lauren Bacall     1924.0     2014.0           actress   \n",
      "4  nm0000002  Lauren Bacall     1924.0     2014.0        soundtrack   \n",
      "\n",
      "                            knownForTitles  \n",
      "0  tt0072308,tt0050419,tt0053137,tt0027125  \n",
      "1  tt0072308,tt0050419,tt0053137,tt0027125  \n",
      "2  tt0072308,tt0050419,tt0053137,tt0027125  \n",
      "3  tt0037382,tt0075213,tt0117057,tt0038355  \n",
      "4  tt0037382,tt0075213,tt0117057,tt0038355  \n",
      "Exploded data saved to: C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_exploded.tsv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the cleaned name.basics file\n",
    "cleaned_file_path = r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_cleaned.tsv'\n",
    "name_basics_cleaned_df = pd.read_csv(cleaned_file_path, sep='\\t', encoding='utf-8', na_values='\\\\N')\n",
    "\n",
    "# Explode the primaryProfession column\n",
    "name_basics_cleaned_df['primaryProfession'] = name_basics_cleaned_df['primaryProfession'].str.split(',')  # Split on comma\n",
    "name_basics_exploded_df = name_basics_cleaned_df.explode('primaryProfession')  # Explode the list into rows\n",
    "\n",
    "# Reset the index if desired\n",
    "name_basics_exploded_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Check the shape and head of the exploded DataFrame\n",
    "print(\"Exploded DataFrame shape:\", name_basics_exploded_df.shape)\n",
    "print(name_basics_exploded_df.head())\n",
    "\n",
    "# Optionally, save the exploded DataFrame if needed\n",
    "exploded_file_path = r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_exploded.tsv'\n",
    "name_basics_exploded_df.to_csv(exploded_file_path, sep='\\t', index=False)\n",
    "\n",
    "print(f'Exploded data saved to: {exploded_file_path}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba7a0724-591f-4821-9861-62536739f4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell E7440\\AppData\\Local\\Temp\\ipykernel_2704\\2523995220.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  name_basics_exploded_df['primaryProfession'].fillna(mode_profession, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in primaryProfession after filling: 0\n",
      "Updated exploded data saved to: C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_exploded_filled.tsv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the exploded name.basics file\n",
    "exploded_file_path = r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_exploded.tsv'\n",
    "name_basics_exploded_df = pd.read_csv(exploded_file_path, sep='\\t', encoding='utf-8', na_values='\\\\N')\n",
    "\n",
    "\n",
    "# Fill missing values with the mode (most common profession)\n",
    "mode_profession = name_basics_exploded_df['primaryProfession'].mode()[0]\n",
    "name_basics_exploded_df['primaryProfession'].fillna(mode_profession, inplace=True)\n",
    "\n",
    "# Check the null values after filling\n",
    "null_counts_after_fill = name_basics_exploded_df['primaryProfession'].isnull().sum()\n",
    "print(f'Null values in primaryProfession after filling: {null_counts_after_fill}')\n",
    "\n",
    "# Save the updated DataFrame back to the Silver layer\n",
    "updated_exploded_file_path = r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_exploded_filled.tsv'\n",
    "name_basics_exploded_df.to_csv(updated_exploded_file_path, sep='\\t', index=False)\n",
    "\n",
    "print(f'Updated exploded data saved to: {updated_exploded_file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20d0b55b-4043-4932-b443-ff64da1e03e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in birthYear after filling: 0\n",
      "Updated exploded data saved to: C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_new.tsv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the updated exploded file\n",
    "updated_exploded_file_path = r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_exploded_filled.tsv'\n",
    "name_basics_exploded_df = pd.read_csv(updated_exploded_file_path, sep='\\t', encoding='utf-8', na_values='\\\\N')\n",
    "\n",
    "\n",
    "# Group by primaryProfession to calculate the median birth year for each profession\n",
    "median_birth_years = name_basics_exploded_df.groupby('primaryProfession')['birthYear'].median()\n",
    "\n",
    "# Function to fill birthYear based on primaryProfession\n",
    "def fill_birth_year(row):\n",
    "    if pd.isnull(row['birthYear']):\n",
    "        return median_birth_years.get(row['primaryProfession'], row['birthYear'])  # Return NaN if no profession found\n",
    "    return row['birthYear']\n",
    "\n",
    "# Apply the function to fill missing birthYear values\n",
    "name_basics_exploded_df['birthYear'] = name_basics_exploded_df.apply(fill_birth_year, axis=1)\n",
    "\n",
    "# Check the count of null values after filling\n",
    "final_birth_year_null_counts = name_basics_exploded_df['birthYear'].isnull().sum()\n",
    "print(f'Null values in birthYear after filling: {final_birth_year_null_counts}')\n",
    "\n",
    "# Save the updated DataFrame back to the Silver layer\n",
    "final_updated_file_path = r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_new.tsv'\n",
    "name_basics_exploded_df.to_csv(final_updated_file_path, sep='\\t', index=False)\n",
    "\n",
    "print(f'Updated exploded data saved to: {final_updated_file_path}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e5b5ebc-4237-4d0e-b7ba-f261c6daffdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in each column:\n",
      "nconst                      0\n",
      "primaryName                 0\n",
      "birthYear                   0\n",
      "deathYear            17763026\n",
      "primaryProfession           0\n",
      "knownForTitles        1615110\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the updated file\n",
    "file_path = r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_new.tsv'\n",
    "name_basics_new_df = pd.read_csv(file_path, sep='\\t', encoding='utf-8', na_values='\\\\N')\n",
    "\n",
    "# Check for null values in all columns\n",
    "null_counts = name_basics_new_df.isnull().sum()\n",
    "\n",
    "# Print the null counts for each column\n",
    "print('Null values in each column:')\n",
    "print(null_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8491ff44-1eca-4297-a533-62508307715d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dask in c:\\users\\dell e7440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2024.9.0)\n",
      "Requirement already satisfied: click>=8.1 in c:\\users\\dell e7440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in c:\\users\\dell e7440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask) (3.0.0)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in c:\\users\\dell e7440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell e7440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask) (24.1)\n",
      "Requirement already satisfied: partd>=1.4.0 in c:\\users\\dell e7440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask) (1.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\dell e7440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask) (6.0.2)\n",
      "Requirement already satisfied: toolz>=0.10.0 in c:\\users\\dell e7440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dask) (0.12.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell e7440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click>=8.1->dask) (0.4.6)\n",
      "Requirement already satisfied: locket in c:\\users\\dell e7440\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from partd>=1.4.0->dask) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install dask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf7c6712-b55e-4099-8381-012308ff36d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploded knownForTitles data saved to: C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_new_exploded.tsv\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Load the updated file using Dask\n",
    "file_path = r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_new.tsv'\n",
    "name_basics_ddf = dd.read_csv(file_path, sep='\\t', encoding='utf-8', na_values='\\\\N')\n",
    "\n",
    "# Explode the knownForTitles column\n",
    "name_basics_ddf['knownForTitles'] = name_basics_ddf['knownForTitles'].str.split(',')\n",
    "name_basics_exploded_ddf = name_basics_ddf.explode('knownForTitles')\n",
    "\n",
    "# Optionally, save the exploded DataFrame if needed\n",
    "exploded_file_path = r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_new_exploded.tsv'\n",
    "name_basics_exploded_ddf.to_csv(exploded_file_path, sep='\\t', index=False, single_file=True)\n",
    "\n",
    "print(f'Exploded knownForTitles data saved to: {exploded_file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4484747a-c57a-4ab2-a3cf-75ab5dc979eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the exploded DataFrame:\n",
      "['nconst', 'primaryName', 'birthYear', 'deathYear', 'primaryProfession', 'knownForTitles']\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Load the exploded file using Dask\n",
    "exploded_file_path = r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_new_exploded.tsv'\n",
    "name_basics_exploded_ddf = dd.read_csv(exploded_file_path, sep='\\t', encoding='utf-8', na_values='\\\\N')\n",
    "\n",
    "# Print the columns of the DataFrame\n",
    "print('Columns in the exploded DataFrame:')\n",
    "print(name_basics_exploded_ddf.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21780f72-be0c-439f-99c7-a98cc7731b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the exploded DataFrame:\n",
      "['nconst', 'primaryName', 'birthYear', 'deathYear', 'primaryProfession', 'knownForTitles']\n",
      "\n",
      "First few rows of the exploded DataFrame:\n",
      "      nconst   primaryName  birthYear  deathYear primaryProfession  \\\n",
      "0  nm0000001  Fred Astaire     1899.0     1987.0             actor   \n",
      "1  nm0000001  Fred Astaire     1899.0     1987.0             actor   \n",
      "2  nm0000001  Fred Astaire     1899.0     1987.0             actor   \n",
      "3  nm0000001  Fred Astaire     1899.0     1987.0             actor   \n",
      "4  nm0000001  Fred Astaire     1899.0     1987.0     miscellaneous   \n",
      "\n",
      "  knownForTitles  \n",
      "0      tt0072308  \n",
      "1      tt0050419  \n",
      "2      tt0053137  \n",
      "3      tt0027125  \n",
      "4      tt0072308  \n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Load the exploded file using Dask\n",
    "exploded_file_path = r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_new_exploded.tsv'\n",
    "name_basics_exploded_ddf = dd.read_csv(exploded_file_path, sep='\\t', encoding='utf-8', na_values='\\\\N')\n",
    "\n",
    "# Print the columns of the DataFrame\n",
    "print('Columns in the exploded DataFrame:')\n",
    "print(name_basics_exploded_ddf.columns.tolist())\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print('\\nFirst few rows of the exploded DataFrame:')\n",
    "print(name_basics_exploded_ddf.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eae415b2-a8fd-4bdf-881b-0e08169bbd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types after conversion:\n",
      "nconst               string[pyarrow]\n",
      "primaryName          string[pyarrow]\n",
      "birthYear                      Int64\n",
      "deathYear                      Int64\n",
      "primaryProfession    string[pyarrow]\n",
      "knownForTitles       string[pyarrow]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convert birthYear and deathYear to integers\n",
    "name_basics_exploded_ddf['birthYear'] = name_basics_exploded_ddf['birthYear'].astype('Int64')  # Using 'Int64' to handle NA\n",
    "name_basics_exploded_ddf['deathYear'] = name_basics_exploded_ddf['deathYear'].astype('Int64')  # Using 'Int64' to handle NA\n",
    "\n",
    "# Check data types again to confirm changes\n",
    "print('Data types after conversion:')\n",
    "print(name_basics_exploded_ddf.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdc4221a-ba91-4c4f-adb9-08a29e738768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated exploded data with filled deathYear saved to: C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_exploded_filled_death_year.tsv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the exploded DataFrame from the specified file\n",
    "exploded_file_path = r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_new_exploded.tsv'\n",
    "name_basics_exploded_df = pd.read_csv(exploded_file_path, sep='\\t', encoding='utf-8', na_values='\\\\N')\n",
    "\n",
    "# Calculate the median deathYear for each primaryProfession\n",
    "profession_medians = name_basics_exploded_df.groupby('primaryProfession')['deathYear'].median()\n",
    "\n",
    "# Fill missing values in deathYear based on the median of primaryProfession\n",
    "name_basics_exploded_df['deathYear'] = name_basics_exploded_df.apply(\n",
    "    lambda row: profession_medians[row['primaryProfession']] if pd.isnull(row['deathYear']) else row['deathYear'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Save the updated DataFrame back to the Silver layer\n",
    "updated_exploded_file_path = r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_exploded_filled_death_year.tsv'\n",
    "name_basics_exploded_df.to_csv(updated_exploded_file_path, sep='\\t', index=False)\n",
    "\n",
    "print(f'Updated exploded data with filled deathYear saved to: {updated_exploded_file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcfd7cea-c06d-4003-9ef6-f3038465c0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in each column:\n",
      "nconst                     0\n",
      "primaryName                0\n",
      "birthYear                  0\n",
      "deathYear               1202\n",
      "primaryProfession          0\n",
      "knownForTitles       1615110\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the updated exploded DataFrame\n",
    "file_path = r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_exploded_filled_death_year.tsv'\n",
    "name_basics_exploded_filled_df = pd.read_csv(file_path, sep='\\t', encoding='utf-8', na_values='\\\\N')\n",
    "\n",
    "# Check for null values in each column\n",
    "null_counts = name_basics_exploded_filled_df.isnull().sum()\n",
    "\n",
    "# Print the null values counts for each column\n",
    "print('Null values in each column:')\n",
    "print(null_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3f2deb5-1ae4-429e-aebc-8495d4118e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell E7440\\AppData\\Local\\Temp\\ipykernel_10800\\1454689211.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  name_basics_exploded_df['knownForTitles'].fillna('Unknown', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated exploded data with filled knownForTitles saved to: C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_exploded_final_knownForTitles_filled.tsv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the DataFrame from the existing exploded file\n",
    "file_path = r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_exploded_filled_death_year.tsv'\n",
    "name_basics_exploded_df = pd.read_csv(file_path, sep='\\t', encoding='utf-8', na_values='\\\\N')\n",
    "\n",
    "# Fill missing values in the 'knownForTitles' column with 'Unknown'\n",
    "name_basics_exploded_df['knownForTitles'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Save the updated DataFrame back to the Silver layer\n",
    "updated_file_path = r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_exploded_final_knownForTitles_filled.tsv'\n",
    "name_basics_exploded_df.to_csv(updated_file_path, sep='\\t', index=False)\n",
    "\n",
    "print(f'Updated exploded data with filled knownForTitles saved to: {updated_file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbd57e95-de26-45f8-b02c-feb2cb224fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in each column:\n",
      " nconst                  0\n",
      "primaryName             0\n",
      "birthYear               0\n",
      "deathYear            1202\n",
      "primaryProfession       0\n",
      "knownForTitles          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path to the updated file\n",
    "file_path = r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_exploded_final_knownForTitles_filled.tsv'\n",
    "\n",
    "# Load the DataFrame\n",
    "df = pd.read_csv(file_path, sep='\\t', encoding='utf-8')\n",
    "\n",
    "# Check for null values in each column\n",
    "null_values = df.isnull().sum()\n",
    "\n",
    "# Print the null values in each column\n",
    "print(\"Null values in each column:\\n\", null_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66bbf62a-42c2-4936-b36a-8c6aed768a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates removed and updated file saved to: C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_exploded_final.tsv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path to the updated file\n",
    "file_path = r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_exploded_final_knownForTitles_filled.tsv'\n",
    "\n",
    "# Load the DataFrame\n",
    "df = pd.read_csv(file_path, sep='\\t', encoding='utf-8')\n",
    "\n",
    "# Drop duplicates across all columns\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Save the updated DataFrame back to the file\n",
    "updated_file_path = r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_exploded_final.tsv'\n",
    "df.to_csv(updated_file_path, sep='\\t', index=False)\n",
    "\n",
    "print(f\"Duplicates removed and updated file saved to: {updated_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66e58422-6112-40f1-a6a3-ef121c220e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'deathYear' column has been removed. Updated file saved to: C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_exploded_final1.tsv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path to the input file\n",
    "file_path = r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_exploded_final.tsv'\n",
    "\n",
    "# Load the DataFrame\n",
    "df = pd.read_csv(file_path, sep='\\t', encoding='utf-8')\n",
    "\n",
    "# Drop the 'deathYear' column\n",
    "df.drop(columns=['deathYear'], inplace=True)\n",
    "\n",
    "# Save the updated DataFrame back to a file\n",
    "updated_file_path = r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_exploded_final1.tsv'\n",
    "df.to_csv(updated_file_path, sep='\\t', index=False)\n",
    "\n",
    "print(f\"'deathYear' column has been removed. Updated file saved to: {updated_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "593992bd-a1f6-43d3-98fe-b2950668fce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the DataFrame:\n",
      "Index(['nconst', 'primaryName', 'birthYear', 'primaryProfession',\n",
      "       'knownForTitles'],\n",
      "      dtype='object')\n",
      "\n",
      "First few rows of the DataFrame:\n",
      "      nconst   primaryName  birthYear primaryProfession knownForTitles\n",
      "0  nm0000001  Fred Astaire     1899.0             actor      tt0072308\n",
      "1  nm0000001  Fred Astaire     1899.0             actor      tt0050419\n",
      "2  nm0000001  Fred Astaire     1899.0             actor      tt0053137\n",
      "3  nm0000001  Fred Astaire     1899.0             actor      tt0027125\n",
      "4  nm0000001  Fred Astaire     1899.0     miscellaneous      tt0072308\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path to the name_basics_exploded_final1.tsv file\n",
    "file_path = r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_exploded_final1.tsv'\n",
    "\n",
    "# Load the DataFrame\n",
    "df = pd.read_csv(file_path, sep='\\t', encoding='utf-8')\n",
    "\n",
    "# Print the columns and the first few rows\n",
    "print(\"Columns in the DataFrame:\")\n",
    "print(df.columns)\n",
    "\n",
    "print(\"\\nFirst few rows of the DataFrame:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1ae32c6-6a36-4e59-8cd7-dae121b7221f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      nconst   primaryName  birthYear primaryProfession knownForTitles\n",
      "0  nm0000001  Fred Astaire       1899             actor      tt0072308\n",
      "1  nm0000001  Fred Astaire       1899             actor      tt0050419\n",
      "2  nm0000001  Fred Astaire       1899             actor      tt0053137\n",
      "3  nm0000001  Fred Astaire       1899             actor      tt0027125\n",
      "4  nm0000001  Fred Astaire       1899     miscellaneous      tt0072308\n",
      "DataFrame saved to C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_exploded_final1.tsv\n"
     ]
    }
   ],
   "source": [
    "#Make birth year an integer\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the DataFrame from the TSV file\n",
    "df = pd.read_csv(r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_exploded_final1.tsv', sep='\\t')\n",
    "\n",
    "# Convert the 'birthYear' column to integer type\n",
    "df['birthYear'] = df['birthYear'].astype(int)\n",
    "\n",
    "# Display the first few rows to confirm the conversion\n",
    "print(df.head())\n",
    "\n",
    "# Save the updated DataFrame to the same TSV file\n",
    "file_path = r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_exploded_final1.tsv'\n",
    "df.to_csv(file_path, sep='\\t', index=False)\n",
    "\n",
    "print(f\"DataFrame saved to {file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adaaa355-0e05-43e5-8fdc-95f9e6c66834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      tconst  ordering     nconst         category                      job  \\\n",
      "0  tt0000001         1  nm1588970             self                      NaN   \n",
      "1  tt0000001         2  nm0005690         director                      NaN   \n",
      "2  tt0000001         3  nm0005690         producer                 producer   \n",
      "3  tt0000001         4  nm0374658  cinematographer  director of photography   \n",
      "4  tt0000002         1  nm0721526         director                      NaN   \n",
      "\n",
      "  characters  \n",
      "0   [\"Self\"]  \n",
      "1        NaN  \n",
      "2        NaN  \n",
      "3        NaN  \n",
      "4        NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path for the title.principals file in the BRONZE layer\n",
    "title_principals_path = r'C:\\Users\\Dell E7440\\Desktop\\BRONZEIMDB\\title.principals.tsv'\n",
    "\n",
    "# Load the title.principals file into a DataFrame with na_values for handling '\\N'\n",
    "title_principals_df = pd.read_csv(title_principals_path, sep='\\t', na_values='\\\\N')\n",
    "\n",
    "# Display the first few rows to confirm successful loading\n",
    "print(title_principals_df.head())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10264c3f-47c1-4511-a9c3-9c9ca432290d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tconst               0\n",
      "ordering             0\n",
      "nconst               0\n",
      "category             0\n",
      "job           71829774\n",
      "characters    45531265\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File path for the title.principals file in the BRONZE layer\n",
    "title_principals_path = r'C:\\Users\\Dell E7440\\Desktop\\BRONZEIMDB\\title.principals.tsv'\n",
    "# Initialize an empty dictionary to hold null counts\n",
    "null_counts = {}\n",
    "\n",
    "# Load the title.principals file in chunks\n",
    "chunk_size = 100000  # Adjust the chunk size as needed\n",
    "for chunk in pd.read_csv(title_principals_path, sep='\\t', na_values='\\\\N', chunksize=chunk_size):\n",
    "    # Count null values for this chunk and update the overall null counts\n",
    "    null_counts_chunk = chunk.isnull().sum()\n",
    "    null_counts = {col: null_counts.get(col, 0) + null_counts_chunk[col] for col in null_counts_chunk.index}\n",
    "\n",
    "# Display the null counts for each column\n",
    "print(pd.Series(null_counts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e9d732-2e6e-4393-95ce-9203fa3b83a0",
   "metadata": {},
   "source": [
    "#SILVER LAYER TRANFORMATIONS FOR TITLE.PRINCIPALS FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b609ecd-b16f-40d3-9cfb-5552537aae64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell E7440\\AppData\\Local\\Temp\\ipykernel_13288\\2305363119.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['job'].fillna('Unknown', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\title_principals_cleaned.csv\n",
      "      tconst  ordering     nconst         category                      job  \\\n",
      "0  tt0000001         1  nm1588970             self                  Unknown   \n",
      "1  tt0000001         2  nm0005690         director                  Unknown   \n",
      "2  tt0000001         3  nm0005690         producer                 producer   \n",
      "3  tt0000001         4  nm0374658  cinematographer  director of photography   \n",
      "4  tt0000002         1  nm0721526         director                  Unknown   \n",
      "\n",
      "  characters  \n",
      "0   [\"Self\"]  \n",
      "1        NaN  \n",
      "2        NaN  \n",
      "3        NaN  \n",
      "4        NaN  \n"
     ]
    }
   ],
   "source": [
    "#Handling null values for job column\n",
    "import pandas as pd\n",
    "\n",
    "# File path for the title.principals file in the BRONZE layer\n",
    "title_principals_path = r'C:\\Users\\Dell E7440\\Desktop\\BRONZEIMDB\\title.principals.tsv'\n",
    "\n",
    "# Load the entire dataset\n",
    "df = pd.read_csv(title_principals_path, sep='\\t', na_values='\\\\N')\n",
    "\n",
    "#Replace missing values in the 'job' column with 'Unknown'\n",
    "df['job'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Save the cleaned data to the SILVER layer\n",
    "silver_path = r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\title_principals_cleaned.csv'\n",
    "df.to_csv(silver_path, index=False)\n",
    "\n",
    "# Print confirmation and a preview of the cleaned DataFrame\n",
    "print(f\"Cleaned data saved to {silver_path}\")\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71e44356-5318-41a7-ba87-55f3b9754176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-cleaned data saved to C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\title_principals_recleaned.csv\n"
     ]
    }
   ],
   "source": [
    "#Handling null values at the characters column\n",
    "import pandas as pd\n",
    "\n",
    "# File path for the cleaned title.principals file in the SILVER layer\n",
    "silver_file_path = r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\title_principals_cleaned.csv'\n",
    "\n",
    "# Specify data types to reduce memory usage (adjust based on your dataset's columns)\n",
    "dtype_spec = {\n",
    "    'tconst': 'object', \n",
    "    'nconst': 'object', \n",
    "    'category': 'object', \n",
    "    'characters': 'object'\n",
    "    # Add more columns with appropriate data types if needed\n",
    "}\n",
    "\n",
    "# Read and process the file in chunks to avoid memory errors\n",
    "chunks = pd.read_csv(silver_file_path, chunksize=50000, dtype=dtype_spec)\n",
    "\n",
    "# Initialize an empty list to store processed chunks\n",
    "chunk_list = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    # 1. Remove the string formatting from the 'characters' column\n",
    "    chunk['characters'] = chunk['characters'].fillna('Unknown').str.strip(\"[]\").str.replace(\"'\", \"\")\n",
    "\n",
    "\n",
    "    # Append the cleaned chunk to the list\n",
    "    chunk_list.append(chunk)\n",
    "\n",
    "# Concatenate all cleaned chunks into a single DataFrame\n",
    "df_cleaned = pd.concat(chunk_list)\n",
    "\n",
    "# Save the re-cleaned data back to the SILVER layer\n",
    "new_silver_file_path = r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\title_principals_recleaned.csv'\n",
    "df_cleaned.to_csv(new_silver_file_path, index=False)\n",
    "\n",
    "# Print confirmation\n",
    "print(f\"Re-cleaned data saved to {new_silver_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75c30083-16b2-4f16-920d-182ee302c316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      tconst  ordering     nconst         category                      job  \\\n",
      "0  tt0000001         1  nm1588970             self                  Unknown   \n",
      "1  tt0000001         2  nm0005690         director                  Unknown   \n",
      "2  tt0000001         3  nm0005690         producer                 producer   \n",
      "3  tt0000001         4  nm0374658  cinematographer  director of photography   \n",
      "4  tt0000002         1  nm0721526         director                  Unknown   \n",
      "\n",
      "  characters  \n",
      "0     \"Self\"  \n",
      "1    Unknown  \n",
      "2    Unknown  \n",
      "3    Unknown  \n",
      "4    Unknown  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the re-cleaned data file\n",
    "file_path = r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\title_principals_recleaned.csv'\n",
    "\n",
    "# Load the CSV file in chunks of 1000 rows\n",
    "chunksize = 1000\n",
    "df_chunk = pd.read_csv(file_path, chunksize=chunksize)\n",
    "\n",
    "# Read the first chunk and print the first 5 rows\n",
    "df = next(df_chunk)\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed5f82cc-a31a-4a94-afeb-5ee51067322b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates removed, 'ordering' column dropped, and the cleaned file has been saved.\n"
     ]
    }
   ],
   "source": [
    "#Drop ordering column because it is not necessary for the futher analysis that wwe will conduct\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "input_file = r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\title_principals_recleaned.csv'\n",
    "output_file = r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\title_principals_no_duplicates.csv'  # New output file name\n",
    "\n",
    "# Process the file in smaller chunks\n",
    "chunksize = 50000  # Try a smaller chunk size\n",
    "\n",
    "# Initialize the output file by writing the header\n",
    "header_written = False\n",
    "\n",
    "# Process the file chunk by chunk\n",
    "for chunk in pd.read_csv(input_file, chunksize=chunksize):\n",
    "    # Drop the 'ordering' column\n",
    "    chunk = chunk.drop(columns=['ordering'])\n",
    "\n",
    "    # Drop duplicates within the chunk\n",
    "    chunk = chunk.drop_duplicates()\n",
    "\n",
    "    # Write to CSV, append mode for subsequent chunks\n",
    "    chunk.to_csv(output_file, mode='a', header=not header_written, index=False)\n",
    "    header_written = True  # Ensure only the first chunk writes the header\n",
    "\n",
    "print(\"Duplicates removed, 'ordering' column dropped, and the cleaned file has been saved.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bd23f76-5e2c-4aa1-8502-1ed38ef57c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      tconst     nconst         category                      job characters\n",
      "0  tt0000001  nm1588970             self                  Unknown     \"Self\"\n",
      "1  tt0000001  nm0005690         director                  Unknown    Unknown\n",
      "2  tt0000001  nm0005690         producer                 producer    Unknown\n",
      "3  tt0000001  nm0374658  cinematographer  director of photography    Unknown\n",
      "4  tt0000002  nm0721526         director                  Unknown    Unknown\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the cleaned CSV file\n",
    "file_path = r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\title_principals_no_duplicates.csv'\n",
    "\n",
    "# Load the cleaned CSV and display the first 5 rows\n",
    "df_cleaned = pd.read_csv(file_path)\n",
    "print(df_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f15d1d59-787e-4081-b1f4-712c089bf081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tconst        0\n",
      "nconst        0\n",
      "category      0\n",
      "job           0\n",
      "characters    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = import pandas as pd\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Count the number of null values in each column\n",
    "null_counts = df.isnull().sum()\n",
    "\n",
    "# Display the count of null values\n",
    "print(null_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e857595e-e1cf-46fc-a083-9ecbac294e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOLD LAYER TRANSFORMATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b63d105-62ee-454c-82e2-0b017b5336e2",
   "metadata": {},
   "source": [
    "#FURTHER AGGREGATIONS OF THE GOLD LAYER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a810c8b9-69db-490a-b8da-c4c0166da326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       nconst  total_titles    primaryName primaryProfession\n",
      "0      nconst             1           <NA>              <NA>\n",
      "1   nm0000001           348   Fred Astaire             actor\n",
      "5   nm0000001           348   Fred Astaire     miscellaneous\n",
      "9   nm0000001           348   Fred Astaire          producer\n",
      "13  nm0000002           499  Lauren Bacall           actress\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Title Principals dataset with specific dtypes to reduce memory usage\n",
    "title_principals_df = pd.read_csv(\n",
    "    r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\title_principals_no_duplicates.csv',\n",
    "    dtype={'tconst': 'string', 'nconst': 'string', 'job': 'string'}\n",
    ")\n",
    "\n",
    "# Load the Name Basics dataset with specific dtypes\n",
    "name_basics_df = pd.read_csv(\n",
    "    r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\name_basics_exploded_final1.tsv',\n",
    "    sep='\\t',\n",
    "    dtype={'nconst': 'string', 'primaryName': 'string', 'primaryProfession': 'string'}\n",
    ")\n",
    "\n",
    "# Count titles associated with each individual (nconst) in chunks\n",
    "title_counts = title_principals_df.groupby('nconst')['tconst'].nunique().reset_index(name='total_titles')\n",
    "\n",
    "# Merge with the Name Basics dataset to get primary names and professions\n",
    "individual_summary = title_counts.merge(\n",
    "    name_basics_df[['nconst', 'primaryName', 'primaryProfession']],\n",
    "    on='nconst',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Remove duplicates in case an individual has multiple professions\n",
    "individual_summary = individual_summary.drop_duplicates()\n",
    "\n",
    "# Save the individual contributions summary as a gold layer file\n",
    "individual_summary.to_csv(r'C:\\Users\\Dell E7440\\Desktop\\GOLDIMDB\\gold_individual_summary.csv', index=False)\n",
    "\n",
    "# Display the summary to verify the output\n",
    "print(individual_summary.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02d0263b-14ae-476e-aebc-3b1ce355c535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      nconst      primaryName  total_titles  \\\n",
      "0  nm0000001     Fred Astaire           348   \n",
      "1  nm0000002    Lauren Bacall           499   \n",
      "2  nm0000003  Brigitte Bardot           230   \n",
      "3  nm0000004     John Belushi           190   \n",
      "4  nm0000005   Ingmar Bergman           243   \n",
      "\n",
      "                      primaryProfession  \n",
      "0        miscellaneous, actor, producer  \n",
      "1  archive_footage, soundtrack, actress  \n",
      "2   producer, music_department, actress  \n",
      "3       actor, music_department, writer  \n",
      "4               actor, director, writer  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the individual contributions summary\n",
    "individual_summary = pd.read_csv(r'C:\\Users\\Dell E7440\\Desktop\\GOLDIMDB\\gold_individual_summary.csv')\n",
    "\n",
    "# Remove duplicates while aggregating total_titles and combining primaryProfession\n",
    "cleaned_summary = individual_summary.groupby(['nconst', 'primaryName']).agg(\n",
    "    total_titles=('total_titles', 'first'),  # Retain the first count of total titles\n",
    "    primaryProfession=('primaryProfession', lambda x: ', '.join(set(x.dropna())))  # Combine unique professions\n",
    ").reset_index()\n",
    "\n",
    "# Save the cleaned summary back to a CSV file\n",
    "cleaned_summary.to_csv(r'C:\\Users\\Dell E7440\\Desktop\\GOLDIMDB\\cleaned_individual_summary.csv', index=False)\n",
    "\n",
    "# Display the cleaned summary to verify the output\n",
    "print(cleaned_summary.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0a3e499-ed3a-4e38-b924-86505f6e9680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           category  number_of_titles\n",
      "0             actor           5642046\n",
      "1           actress           5147685\n",
      "2   archive_footage            167259\n",
      "3     archive_sound              3904\n",
      "4  casting_director            779974\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Title Principals dataset\n",
    "title_principals_df = pd.read_csv(r'C:\\Users\\Dell E7440\\Desktop\\SILVERIMDB\\title_principals_no_duplicates.csv')\n",
    "\n",
    "# Count titles per category\n",
    "title_category_summary = title_principals_df.groupby('category')['tconst'].nunique().reset_index(name='number_of_titles')\n",
    "\n",
    "# Save the title category summary as a gold layer file\n",
    "title_category_summary.to_csv(r'C:\\Users\\Dell E7440\\Desktop\\GOLDIMDB\\gold_title_category_summary.csv', index=False)\n",
    "\n",
    "# Display the summary to verify the output\n",
    "print(title_category_summary.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d5e2e9-8eca-469b-b313-a3dc8ab1c367",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
